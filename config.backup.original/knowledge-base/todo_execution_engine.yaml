# TODO-EXECUTION ENGINE
# Comprehensive system for converting analysis results into TodoWrite plans with automatic Task() delegation

metadata:
  name: "todo_execution_engine"
  version: "0.0.1"
  description: "Automatic TodoWrite-to-Task() delegation system with intelligent execution management"
  author: "master"
  tags: ["todo-execution", "task-delegation", "automatic-execution", "progress-tracking"]

# Core Engine Architecture
todo_execution_engine:
  purpose: |
    Convert TF-IDF analysis results into actionable TodoWrite plans
    and execute each TODO item through optimized Task() delegation
    with real-time progress tracking and intelligent failure recovery

  input_sources:
    - task_analysis_results: "From config/knowledge-base/task-analysis.yaml"
    - tfidf_categorization: "From config/knowledge-base/tfidf-system.yaml"
    - agent_selection_results: "From config/knowledge-base/unified_agent_selection.yaml"
    - clarification_responses: "From config/knowledge-base/clarification_system.yaml"
    - parallel_coordination_data: "From config/knowledge-base/parallel_coordination.yaml"

  output_targets:
    - todo_write_plans: "Structured TodoWrite lists for execution"
    - task_delegation_commands: "Task() calls with optimized parameters"
    - progress_tracking_data: "Real-time execution status updates"
    - result_synthesis_inputs: "Completed task results for integration"

# TodoWrite Plan Generation
todo_plan_generation:
  implementation: |
    # LLM-Interpretable TODO Plan Generation Algorithm
    INPUT: task_analysis, tfidf_results, agent_mapping, complexity_score
    OUTPUT: todo_write_plan, execution_strategy

    LLM_PROCESSING_INSTRUCTIONS:
    1. Analyze Task Structure:
       - Parse task_analysis.complexity_assessment
       - Extract tfidf_results.categorization_vector
       - Review agent_mapping.compatibility_matrix
       - Identify domain_specialization_requirements
       - Determine parallel_execution_possibilities

    2. Task Decomposition Logic:
       IF complexity_score >= 4:
          - Create multi_phase_execution_structure
          - Identify dependency_graph_components
          - Generate parallel_execution_blocks
          - Establish critical_path_sequence
          - Create cross_phase_integration_points
       ELIF complexity_score >= 2:
          - Generate structured_task_sequence (3-7 tasks)
          - Identify parallel_execution_opportunities
          - Create dependency_relationships
          - Set priority_level_assignments
       ELSE:
          - Create simple_linear_plan (1-2 tasks)
          - Sequential_execution_only
          - Single_agent_assignment

    3. TodoWrite Item Construction:
       FOR each identified_subtask:
          todo_item_structure = {
            "content": extract_actionable_description(subtask),
            "status": "pending",
            "activeForm": generate_verb_phrase(subtask.purpose),
            "agent_assignment": select_optimal_agent(subtask.requirements),
            "estimated_time": calculate_time_estimate(subtask.complexity),
            "dependencies": identify_dependency_list(subtask),
            "priority": assign_priority_level(subtask.importance),
            "required_capabilities": extract_capability_requirements(subtask)
          }

    2. Agent Assignment:
       FOR each task in plan:
          - Use dynamic agent discovery for comprehensive agent selection
          - Apply competency vector matching for precise skill alignment
          - Integrate TF-IDF analysis with competency scoring
          - Use dynamic registry to find best-matched agent
          - Apply domain system enhancements for optimal selection
          - Set fallback agent options with master as last resort

    3. TodoWrite Structure:
       todo_items = []
       FOR each task:
          todo_item = {
            "content": "task_description",
            "status": "pending",
            "activeForm": "action_description",
            "agent_assignment": "selected_agent",
            "estimated_time": "time_estimate",
            "dependencies": ["dependency_list"],
            "priority": "high/medium/low"
          }
          todo_items.append(todo_item)

  execution_strategies:
    simple_sequential:
      complexity_range: [1, 2]
      todo_structure: "linear_task_list"
      execution_pattern: "task1 → task2 → task3"
      parallel_support: false

    mixed_execution:
      complexity_range: [3, 4]
      todo_structure: "phased_execution_with_parallel_blocks"
      execution_pattern: "phase1(parallel) → phase2(sequential) → phase3(parallel)"
      parallel_support: true

    complex_coordination:
      complexity_range: [5, 10]
      todo_structure: "multi_layered_dependency_graph"
      execution_pattern: "dynamic_execution_with_adaptive_coordination"
      parallel_support: true
      dependency_tracking: true

# Task() Delegation System
task_delegation_system:
  implementation: |
    # LLM-Interpretable Task() Delegation Algorithm
    INPUT: todo_item, agent_assignment, execution_context
    OUTPUT: delegation_result, execution_status

    LLM_DELEGATION_INSTRUCTIONS:
    1. Pre-execution Validation Protocol:
       - Check agent_availability_status in dynamic_agent_registry
       - Validate Task() system_readiness through mcp_registry
       - Verify task_requirements_clarity from todo_item.content
       - Confirm resource_allocation_sufficiency
       - Assess dependency_satisfaction_status

    2. Task() Parameter Construction Algorithm:
       delegation_parameters = {
         "subagent_type": resolve_agent_identifier(todo_item.agent_assignment),
         "description": generate_concise_task_description(todo_item),
         "prompt": construct_comprehensive_execution_prompt(todo_item, execution_context)
       }

       PROMPT_CONSTRUCTION_RULES:
       - Include original_task_context from execution_context
       - Add tfidf_analysis_results for agent guidance
       - Specify expected_output_format requirements
       - Include dependency_information if relevant
       - Set quality_assessment_criteria
       - Define success_evaluation_metrics

    3. Execution Protocol with Error Handling:
       LLM_EXECUTION_STEPS:
       1. Initialize Task() call with constructed parameters
       2. Monitor execution_progress through performance_tracking
       3. Update todo_item.status to "in_progress"
       4. Capture execution_result with metadata
       5. Update todo_item.status to "completed" if successful
       6. Log performance_metrics for system_learning

       ERROR_HANDLING_PROTOCOL:
       IF delegation_failure_detected:
          - Classify_failure_type (agent_unavailable, task_complexity, system_error)
          - Apply appropriate_recovery_strategy
          - Update todo_item.status with failure_context
          - Initiate fallback_execution_procedure
          RETURN delegation_result_with_error_context

    4. Post-execution Processing Workflow:
       - Validate result_quality against predefined_criteria
       - Update TodoWrite_progress_status
       - Check and trigger dependent_tasks if conditions_met
       - Record performance_metrics in performance_tracking
       - Update agent_performance_data for future_selections
       - Extract learning_data for system_improvement

  # Dynamic Agent Selection Integration
# Integration with existing dynamic_agent_discovery.yaml system
agent_selection_from_registry:
  implementation: |
    # Use existing dynamic agent discovery system instead of static mapping
    # Reference: config/dynamic/dynamic_agent_discovery.yaml

    def determine_agent_type(task_requirements):
      """
      Dynamic agent type determination using existing registry
      """
      # 1. Get available agents from dynamic discovery system
      available_agents = get_available_agents()  # From dynamic_agent_discovery.yaml

      # 2. Create competency vectors for precise matching
      competency_data = create_competency_vectors(available_agents)
      competency_list = competency_data['competency_list']
      agent_vectors = competency_data['agent_vectors']

      # 3. Create task requirement vector
      task_vector = create_task_requirement_vector(task_requirements, competency_list)

      # 4. Find best agent match using competency vector similarity
      best_match = find_best_agent_match(task_vector, agent_vectors)

      # 5. Apply domain system enhancements if available
      if DOMAIN_SYSTEM_INTEGRATION_ENABLED:
        best_match = integrate_with_domain_system(best_match)

      # 6. Return canonical agent name
      return best_match['agent_name'] if best_match else "master"

    def create_task_requirement_vector(task_requirements, competency_list):
      """
      Create competency vector for task requirements
      """
      vector = [0] * len(competency_list)
      task_caps = task_requirements.get('required_capabilities', [])

      for i, comp in enumerate(competency_list):
        if comp in task_caps:
          vector[i] = 1

      return vector

    def find_best_agent_match(task_vector, agent_vectors):
      """
      Find best agent match using vector similarity
      """
      best_agent = None
      best_score = 0.0

      for agent_data in agent_vectors:
        # Calculate cosine similarity
        agent_vector = agent_data['vector']
        similarity = calculate_vector_similarity(task_vector, agent_vector)

        # Apply confidence boost
        confidence = agent_data.get('confidence', 0.7)
        adjusted_score = similarity * confidence

        if adjusted_score > best_score:
          best_score = adjusted_score
          best_agent = agent_data

      return best_agent

    def calculate_vector_similarity(vec1, vec2):
      """
      Calculate cosine similarity between two vectors
      """
      import math

      dot_product = sum(a * b for a, b in zip(vec1, vec2))
      magnitude1 = math.sqrt(sum(a * a for a in vec1))
      magnitude2 = math.sqrt(sum(b * b for b in vec2))

      if magnitude1 == 0 or magnitude2 == 0:
        return 0.0

      return dot_product / (magnitude1 * magnitude2)

    def determine_agent_type_from_registry(todo_item, execution_context):
      """
      Wrapper function to integrate TODO execution with dynamic registry
      """
      # Extract task requirements from TODO item
      task_requirements = {
        'required_capabilities': todo_item.get('required_capabilities', []),
        'domain_specialization': todo_item.get('domain', 'general'),
        'complexity_score': todo_item.get('complexity', 1),
        'urgency': todo_item.get('urgency', 'normal')
      }

      # Extract capabilities from execution context if available
      if execution_context and 'analysis_results' in execution_context:
        analysis_caps = execution_context['analysis_results'].get('detected_capabilities', [])
        task_requirements['required_capabilities'].extend(analysis_caps)

      # Call dynamic selection system
      return determine_agent_type(task_requirements)

# Progress Tracking System
progress_tracking:
  implementation: |
    # LLM-Interpretable Real-time Progress Tracking Algorithm
    INPUT: active_todo_items, execution_results, system_events
    OUTPUT: progress_summary, status_updates, completion_metrics

    LLM_TRACKING_INSTRUCTIONS:
    1. Initialize Tracking Data Structures:
       execution_state_monitor = {
         "total_task_count": calculate_total_tasks(todo_list),
         "completed_task_count": count_completed_tasks(),
         "failed_task_count": count_failed_tasks(),
         "in_progress_task_count": count_active_tasks(),
         "blocked_task_count": count_blocked_tasks(),
         "overall_progress_percentage": calculate_completion_percentage(),
         "estimated_completion_timestamp": project_completion_time(),
         "current_execution_phase": identify_current_phase(),
         "execution_velocity": calculate_tasks_per_hour()
       }

       individual_task_tracking = {}
       FOR each todo_item in active_todo_items:
          individual_task_tracking[todo_item.id] = {
            "current_status": todo_item.status,
            "execution_start_time": todo_item.start_timestamp,
            "execution_end_time": todo_item.completion_timestamp,
            "assigned_agent_identifier": todo_item.agent_assignment,
            "execution_attempt_count": todo_item.retry_count,
            "result_quality_assessment": todo_item.quality_score,
            "dependency_satisfaction_status": check_dependencies(todo_item),
            "estimated_remaining_time": calculate_remaining_time(todo_item)
          }

    2. Real-time Progress Monitoring Protocol:
       CONTINUOUS_MONITORING_TASKS:
       - Monitor TodoWrite status changes in real-time
       - Calculate overall_progress_percentage dynamically
       - Update estimated_completion_time based on current_velocity
       - Detect execution_bottlenecks or performance_issues
       - Track agent_performance_by_task_type
       - Monitor system_resource_utilization

    3. Dependency Management Algorithm:
       DEPENDENCY_TRACKING_LOGIC:
       FOR each pending_task:
          - Check dependency_satisfaction_status
          - Identify blocking_dependencies
          - Calculate dependency_resolution_probability
          - Update task_readiness_status
          - Notify when dependencies become satisfied

       DEPENDENCY_RESOLUTION_TRIGGER:
       WHEN dependency_task COMPLETES:
          - Update all dependent_tasks
          - Recalculate execution_readiness
          - Trigger eligible_tasks for execution
          - Update execution_sequence if needed

    4. Performance Metrics Collection:
       AGENT_PERFORMANCE_TRACKING:
       FOR each agent_type:
          - Calculate task_completion_success_rate
          - Measure average_execution_time_by_complexity
          - Track result_quality_scores
          - Monitor retry_frequency and failure_patterns
          - Update competency_vectors based on performance

       SYSTEM_PERFORMANCE_ANALYSIS:
       - Track overall_delegation_success_rate
       - Monitor parallel_execution_efficiency
       - Calculate resource_utilization_optimization
       - Measure user_satisfaction_indicators
       - Analyze learning_curve_improvement

    PROGRESS_MONITORING:
    1. Real-time Updates:
       - Update TodoWrite after each task completion
       - Calculate overall progress percentage
       - Estimate remaining time based on current velocity
       - Detect potential bottlenecks or delays

    2. Dependency Tracking:
       - Monitor task dependencies
       - Automatically unblock tasks when dependencies complete
       - Handle circular dependency detection
       - Optimize execution order based on dependencies

    3. Performance Metrics:
       - Track Task() delegation success rates
       - Monitor agent performance by task type
       - Calculate execution velocity by complexity
       - Record failure patterns and recovery success

# Intelligent Failure Recovery
failure_recovery_system:
  implementation: |
    # Intelligent Failure Recovery Algorithm
    INPUT: failed_todo_item, failure_context, available_recovery_options
    OUTPUT: recovery_strategy, recovery_execution_result

    FAILURE_CLASSIFICATION:
    1. Agent-related Failures:
       - Agent unavailable → Select alternative agent
       - Agent capability mismatch → Re-analyze requirements
       - Agent timeout → Increase timeout or select faster agent

    2. Task-related Failures:
       - Ambiguous requirements → Trigger clarification workflow
       - Insufficient resources → Scale down task or wait for resources
       - Dependency failures → Resolve dependencies first

    3. System-related Failures:
       - Task() system unavailable → Fallback to direct execution
       - Network issues → Retry with exponential backoff
       - Resource exhaustion → Free resources or simplify task

    RECOVERY_STRATEGIES:
    1. Agent Substitution:
       SELECT alternative_agent FROM available_agents
       WHERE agent.capabilities CONTAINS failed_task.requirements
       ORDER BY agent.success_rate DESC, agent.performance_score DESC
       LIMIT 1

    2. Task Decomposition:
       IF task.complexity > threshold:
          subtask_list = decompose_task(failed_task)
          FOR subtask in subtask_list:
             execute_subtask_with_higher_priority(subtask)

    3. Escalation Handling:
       IF failure_count > max_retry_attempts:
          escalate_to_human_operator(failed_task, failure_context)
          OR mark_task_as_blocked(failed_task)

    4. Adaptive Learning:
       UPDATE failure_patterns_database(failure_context, recovery_result)
       UPDATE agent_performance_metrics(failed_agent, failure_type)
       ADJUST future_agent_selection_weights()

# Result Synthesis Integration
result_synthesis_integration:
  implementation: |
    # Result Synthesis for Completed TODO Lists
    INPUT: completed_task_results, original_request_context, execution_metadata
    OUTPUT: synthesized_result, quality_assessment, performance_report

    SYNTHESIS_PROCESS:
    1. Result Collection:
       - Gather all completed task results
       - Validate result quality and completeness
       - Standardize result formats for processing
       - Identify gaps or inconsistencies

    2. Quality Assessment:
       FOR each result:
          quality_score = calculate_result_quality(result, original_requirements)
          completeness_score = assess_result_completeness(result, expected_scope)
          consistency_score = check_result_consistency(result, other_results)

    3. Synthesis Strategy Selection:
       IF results_have_conflicts:
          USE conflict_resolution_synthesis()
       ELIF results_are_complementary:
          USE complementary_integration_synthesis()
       ELSE:
          USE straightforward_aggregation_synthesis()

    4. Final Result Construction:
       synthesized_result = {
         "main_response": primary_content,
         "supporting_details": supplementary_information,
         "confidence_assessment": overall_confidence_score,
         "quality_metrics": quality_assessment_data,
         "execution_summary": performance_summary
       }

# Enhanced Integration with TF-IDF and Agent Selection Systems
tfidf_integration_engine:
  implementation: |
    # LLM-Interpretable TF-IDF Integration for TODO Planning
    INPUT: tfidf_analysis_results, task_categorization_vector, similarity_scores
    OUTPUT: todo_plan_optimization, agent_assignment_enhancement

    LLM_TFIDF_PROCESSING_INSTRUCTIONS:
    1. Extract TF-IDF Analysis Components:
       tfidf_data_extraction = {
         "primary_categories": extract_top_categories(tfidf_results.category_vector, threshold=0.7),
         "secondary_categories": extract_secondary_categories(tfidf_results.category_vector, threshold=0.4),
         "semantic_similarity_scores": tfidf_results.similarity_matrix,
         "keyword_relevance_weights": calculate_keyword_importance(tfidf_results.keyword_scores),
         "domain_confidence_levels": assess_domain_certainty(tfidf_results.confidence_scores)
       }

    2. TODO Plan Optimization Based on TF-IDF:
       PLAN_ENHANCEMENT_ALGORITHM:
       FOR each identified_category in tfidf_data_extraction.primary_categories:
          - Create category_specific_todo_blocks
          - Assign category_optimized_execution_strategy
          - Set priority_based_on_relevance_weight
          - Determine parallel_execution_possibilities within_category

       CROSS_CATEGORY_COORDINATION:
       - Identify interdependencies_between_categories
       - Create cross_category_execution_sequence
       - Optimize resource_allocation_by_category_relevance
       - Establish category_handoff_protocols

    3. Agent Selection Enhancement with TF-IDF:
       TFIDF_AGENT_MATCHING_ALGORITHM:
       FOR each todo_item:
          - Extract tfidf_requirements_vector from todo_item.content
          - Match against agent_competency_vectors using cosine_similarity
          - Apply tfidf_weight_adjustment to agent_selection_scores
          - Consider historical_performance_by_category
          - Generate ranked_agent_list_with_confidence_scores

       SELECTION_CONFIDENCE_CALCULATION:
       agent_confidence_score = (
         tfidf_similarity_weight * 0.6 +
         competency_match_weight * 0.25 +
         historical_performance_weight * 0.10 +
         domain_specialization_weight * 0.05
       )

unified_agent_selection_integration:
  implementation: |
    # Enhanced Integration with Unified Agent Selection System
    INPUT: todo_item_list, unified_selection_results, tfidf_enhancement_data
    OUTPUT: optimized_agent_assignments, delegation_readiness_assessment

    LLM_AGENT_SELECTION_PROCESSING:
    1. Merge Selection System Outputs:
       selection_integration = {
         "unified_selection_scores": unified_selection_results.compatibility_matrix,
         "tfidf_enhancement_scores": tfidf_enhancement_data.agent_confidence_scores,
         "performance_history_data": extract_agent_performance_metrics(),
         "domain_expertise_mapping": map_domain_specializations(),
         "availability_status": check_current_agent_availability()
       }

    2. Optimized Agent Assignment Algorithm:
       FOR each todo_item in todo_item_list:
          - Calculate composite_agent_score = weighted_average_of_all_sources
          - Apply task_complexity_adjustment to scoring
          - Consider parallel_execution_compatibility
          - Validate agent_capability_sufficiency
          - Assign primary_agent_with_fallback_options

       DELEGATION_READINESS_ASSESSMENT:
       - Verify agent_availability_for_assigned_tasks
       - Check resource_allocation_requirements
       - Validate dependency_satisfaction_status
       - Assess execution_risk_factors
       - Generate contingency_plans_for_high_risk_assignments

    3. Dynamic Selection Optimization:
       SELECTION_ADAPTATION_LOGIC:
       - Monitor real_time_agent_performance
       - Adjust selection_weights_based_on_success_rates
       - Update competency_vectors_from_execution_results
       - Refine tfidf_category_mappings_based_on_outcomes
       - Optimize for multi_agent_coordination_efficiency

  with_parallel_coordination:
    trigger: "parallel_execution_required"
    input_data: "dependency_graph, parallel_blocks"
    output_data: "execution_sequence_with_task_delegation"
    coordination: "coordinate parallel Task() execution"

  with_clarification_system:
    trigger: "clarification_completed"
    input_data: "clarified_requirements, refined_context"
    output_data: "updated_todo_plan_with_clarified_tasks"
    coordination: "regenerate todo plan based on clarification"

  with_performance_tracking:
    trigger: "task_execution_completed"
    input_data: "execution_metrics, quality_scores"
    output_data: "updated_agent_performance_data"
    coordination: "continuous learning and optimization"

# Quality Assurance and Monitoring
quality_assurance:
  validation_gates:
    pre_execution_validation:
      - agent_availability_check: "Verify selected agents are available"
      - task_requirements_clarity: "Ensure task descriptions are clear"
      - resource_sufficiency: "Confirm adequate resources for execution"
      - dependency_satisfaction: "Verify all dependencies are resolvable"

    post_execution_validation:
      - result_quality_check: "Validate Task() execution results"
      - todo_status_consistency: "Ensure TodoWrite status is accurate"
      - performance_metric_recording: "Record execution performance data"
      - learning_data_extraction: "Extract data for system improvement"

    synthesis_validation:
      - result_completeness: "Ensure all aspects are covered"
      - consistency_check: "Verify no contradictions in results"
      - user_satisfaction_potential: "Assess likely user satisfaction"
      - system_learning_integration: "Update system models"

  monitoring_metrics:
    execution_efficiency:
      - task_completion_rate: "Percentage of tasks completed successfully"
      - average_execution_time: "Mean time per task completion"
      - agent_utilization: "How effectively agents are utilized"
      - parallel_efficiency: "Speed improvement from parallel execution"

    quality_metrics:
      - result_quality_score: "Average quality of completed results"
      - user_satisfaction_prediction: "Predicted user satisfaction"
      - system_accuracy: "Accuracy of agent selection and task routing"
      - learning_effectiveness: "Improvement rate over time"

    reliability_metrics:
      - system_uptime: "Percentage of time system is operational"
      - failure_recovery_success: "Success rate of failure recovery"
      - task_delegation_reliability: "Reliability of Task() delegation"
      - error_rate: "Rate of errors in execution"

# Configuration and Tuning
configuration_parameters:
  execution_thresholds:
    min_complexity_for_todo_planning: 2
    max_parallel_tasks: 5
    max_retry_attempts: 3
    task_timeout_seconds: 300
    quality_threshold_min: 0.7

  learning_rates:
    agent_performance_update_rate: 0.1
    tfidf_weight_adjustment_rate: 0.05
    failure_pattern_learning_rate: 0.15
    user_feedback_integration_rate: 0.2

  performance_targets:
    task_completion_success_rate: 0.95
    average_task_completion_time: 180  # seconds
    user_satisfaction_target: 0.85
    system_overhead_target: 0.1  # 10% overhead acceptable

# Error Handling and Edge Cases
error_handling:
  critical_system_failures:
    task_system_unavailable:
      fallback_strategy: "direct_execution_with_native_capabilities"
      escalation_trigger: "multiple_continuous_failures"
      recovery_actions: ["restart_task_system", "fallback_to_native", "escalate_to_human"]

    agent_discovery_failure:
      fallback_strategy: "use_master_agent_directly"
      recovery_actions: ["reload_agent_configurations", "use_master_agent", "manual_agent_selection"]

    configuration_corruption:
      fallback_strategy: "use_last_known_good_configuration"
      recovery_actions: ["restore_from_backup", "reinitialize_system", "safe_mode_operation"]

  edge_case_handling:
    ambiguous_task_requirements:
      handling_strategy: "trigger_clarification_workflow"
      user_interaction: "request_specific_requirements"
      system_behavior: "pause_execution_until_clarified"

    circular_dependencies:
      detection_method: "dependency_graph_analysis"
      resolution_strategy: "break_circular_dependency_arbitrarily"
      user_notification: "inform_about_dependency_resolution"

    resource_exhaustion:
      detection_method: "monitor_resource_usage"
      handling_strategy: "scale_down_execution_or_queue_tasks"
      recovery_actions: ["free_unused_resources", "prioritize_critical_tasks"]

# Documentation and Examples
usage_examples:
  simple_task_execution:
    input: "Fix CSS styling issue"
    todo_plan_generated:
      - content: "Analyze CSS styling requirements"
        status: "pending"
        agent_assignment: "frontend-developer"
        estimated_time: "5 minutes"
      - content: "Implement CSS fixes"
        status: "pending"
        agent_assignment: "frontend-developer"
        estimated_time: "10 minutes"
    execution_flow: "sequential_task_delegation"

  complex_project_coordination:
    input: "Implement authentication system"
    todo_plan_generated:
      phase_1_parallel:
        - content: "Design authentication architecture"
          agent_assignment: "backend-architect"
        - content: "Design login UI components"
          agent_assignment: "frontend-developer"
      phase_2_sequential:
        - content: "Implement JWT middleware"
          agent_assignment: "backend-developer"
          dependencies: ["architecture_design_complete"]
        - content: "Implement login forms"
          agent_assignment: "frontend-developer"
          dependencies: ["ui_components_design_complete"]
    execution_flow: "phased_execution_with_parallel_and_sequential_tasks"

  research_and_synthesis:
    input: "Research latest React best practices"
    todo_plan_generated:
      - content: "Search React documentation and updates"
        agent_assignment: "deep-research-agent"
      - content: "Analyze community best practices"
        agent_assignment: "deep-research-agent"
      - content: "Synthesize findings into recommendations"
        agent_assignment: "technical-writer"
        dependencies: ["research_complete"]
    execution_flow: "parallel_research_sequential_synthesis"