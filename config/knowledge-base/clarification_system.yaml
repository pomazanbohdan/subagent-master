# Interactive Clarification System
# Comprehensive clarification algorithms with adaptive question generation and response processing

metadata:
  name: "clarification-system"
  version: "0.0.1"
  description: "Interactive clarification system with contextual question generation and adaptive workflows"
  author: "Master Agent System v0.0.1"

# Core Clarification Algorithm
clarification_algorithm:
  name: "adaptive_contextual_clarification"
  description: "Generate contextual questions and process user responses for task clarification"
  
  input_schema:
    task_description:
      type: "string"
      description: "Ambiguous or unclear task description"
      required: true
    ambiguity_score:
      type: "number"
      description: "Detected ambiguity level (0.0-1.0)"
      required: true
    domain_context:
      type: "object"
      description: "Domain-specific context and constraints"
      required: false
    task_complexity:
      type: "number"
      description: "Task complexity score (1-5)"
      required: true
  
  output_schema:
    clarification_status:
      type: "string"
      enum: ["clarified", "needs_more_info", "escalated"]
    contextual_questions:
      type: "array"
      items:
        type: "object"
        properties:
          question: "string"
          question_type: "string"
          priority: "number"
          context_source: "string"
    refined_task_context:
      type: "object"
      description: "Updated task context after clarification"
    confidence_improvement:
      type: "number"
      description: "Measured improvement in task understanding"

# Ambiguity Detection System
ambiguity_detection:
  enabled: true
  
  detection_criteria:
    vague_language:
      indicators: ["maybe", "possibly", "might", "could", "should", "think about"]
      weight: 0.3
      
    missing_requirements:
      indicators: ["unclear scope", "undefined deliverables", "missing constraints"]
      weight: 0.4
      
    conflicting_goals:
      indicators: ["contradictory requirements", "mutually exclusive outcomes"]
      weight: 0.5
      
    insufficient_context:
      indicators: ["no domain specified", "missing technical context", "unclear environment"]
      weight: 0.3
      
  scoring_algorithm:
    implementation: |
      # Ambiguity Scoring Algorithm
      INPUT: task_description, domain_knowledge, context_analysis
      OUTPUT: ambiguity_score (0.0-1.0)
      
      ALGORITHM:
      1. Linguistic Analysis:
          - Detect vague language patterns
          - Identify uncertain terminology
          - Count ambiguous phrases
          
      2. Context Analysis:
          - Evaluate domain specificity
          - Check for technical context
          - Assess environmental clarity
          
      3. Requirement Analysis:
          - Identify missing deliverables
          - Detect undefined constraints
          - Evaluate scope clarity
          
      4. Scoring Calculation:
          ambiguity_score = (vague_language_score * 0.3) + 
                          (missing_requirements_score * 0.4) + 
                          (conflicting_goals_score * 0.5) + 
                          (insufficient_context_score * 0.3)
                          
      NORMALIZATION: Clamp scores to 0.0-1.0 range
      THRESHOLD: Trigger clarification at ambiguity_score > 0.6

# Contextual Question Generation
question_generation:
  enabled: true
  
  question_types:
    requirements_clarification:
      description: "Questions to clarify specific requirements and deliverables"
      template_patterns:
        - "What specific {deliverable_type} are you looking for?"
        - "Could you describe the expected {outcome_characteristics}?"
        - "What are the acceptance criteria for this {task_component}?"
      context_sources: ["task_description", "domain_knowledge"]
      
    scope_definition:
      description: "Questions to define task boundaries and scope"
      template_patterns:
        - "What should be included/excluded from the scope?"
        - "Are there any specific {constraints} we should consider?"
        - "What are the boundaries of this {task_domain}?"
      context_sources: ["task_complexity", "domain_context"]
      
    technical_context:
      description: "Questions to understand technical requirements and environment"
      template_patterns:
        - "What {technology_stack} should be used?"
        - "Are there any {technical_constraints} we need to follow?"
        - "What's the target {platform_environment}?"
      context_sources: ["domain_system", "environment_detection"]
      
    preference_elicitation:
      description: "Questions to understand user preferences and priorities"
      template_patterns:
        - "Do you prefer {approach_a} or {approach_b}?"
        - "What's more important: {priority_a} or {priority_b}?"
        - "Are there any {stylistic_preferences} we should follow?"
      context_sources: ["user_history", "task_patterns"]
      
    domain_specific:
      description: "Domain-specific questions based on detected domain"
      template_patterns:
        - "What {domain_specific_aspect} should we focus on?"
        - "Are there any {domain_requirements} specific to your industry?"
        - "What {domain_standards} should we follow?"
      context_sources: ["domain_system", "domain_mapping"]
  
  generation_algorithm:
    implementation: |
      # Contextual Question Generation Algorithm
      INPUT: task_description, ambiguity_analysis, domain_context, user_history
      OUTPUT: contextual_questions with priorities
      
      ALGORITHM:
      1. Question Type Selection:
          - Analyze ambiguity sources
          - Select appropriate question types
          - Prioritize based on ambiguity weights
          
      2. Template Adaptation:
          - Select relevant templates for each question type
          - Adapt templates with task-specific context
          - Personalize based on user history
          
      3. Context Integration:
          - Incorporate domain-specific terminology
          - Reference detected domain requirements
          - Consider environmental constraints
          
      4. Prioritization:
          - Score questions based on ambiguity resolution potential
          - Sort by priority and expected impact
          - Limit to top 5-7 questions to avoid overwhelm
          
      5. Quality Filtering:
          - Remove redundant questions
          - Ensure clarity and specificity
          - Validate relevance to task ambiguity

# Response Processing System
response_processing:
  enabled: true
  
  processing_algorithm:
    implementation: |
      # User Response Processing Algorithm
      INPUT: user_responses, original_questions, task_context
      OUTPUT: refined_task_context, confidence_improvement
      
      ALGORITHM:
      1. Response Analysis:
          - Extract key information from responses
          - Identify constraint specifications
          - Detect preference indications
          
      2. Context Integration:
          - Update task description with clarified details
          - Add newly identified constraints
          - Incorporate user preferences
          
      3. Ambiguity Re-evaluation:
          - Re-calculate ambiguity scores
          - Identify remaining uncertainties
          - Assess need for follow-up questions
          
      4. Confidence Calculation:
          - Measure improvement in task understanding
          - Calculate clarity improvement percentage
          - Validate against confidence thresholds
          
      5. Context Refinement:
          - Update domain mappings
          - Refine complexity assessments
          - Adjust agent selection criteria

# Adaptive Clarification Workflows
adaptive_workflows:
  simple_clarification:
    trigger_conditions:
      - "ambiguity_score between 0.6-0.7"
      - "task_complexity <= 2"
      - "domain_confidence >= 0.7"
      
    workflow:
      - generate_3_5_questions: "focus on basic requirements"
      - process_user_responses: "single round clarification"
      - validate_clarification: "check if ambiguity_score < 0.3"
      - proceed_to_execution: "if clarified, else escalate"
      
  comprehensive_clarification:
    trigger_conditions:
      - "ambiguity_score between 0.7-0.9"
      - "task_complexity between 3-4"
      - "domain_confidence between 0.4-0.7"
      
    workflow:
      - generate_5_7_questions: "multi-faceted clarification"
      - process_initial_responses: "analyze first round responses"
      - generate_followup_questions: "target remaining ambiguities"
      - process_followup_responses: "complete clarification"
      - validate_comprehensive_clarification: "thorough assessment"
      - proceed_to_adaptive_execution: "if clarified, else escalate"
      
  expert_clarification:
    trigger_conditions:
      - "ambiguity_score > 0.9"
      - "task_complexity = 5"
      - "domain_confidence < 0.4"
      
    workflow:
      - initiate_expert_consultation: "escalate to domain specialist"
      - multi_stage_clarification: "iterative refinement process"
      - stakeholder_coordination: "involve multiple perspectives"
      - validation_workshop: "comprehensive review process"
      - final_validation: "expert confirmation of clarity"
      
  domain_specific_clarification:
    trigger_conditions:
      - "domain_specific_ambiguity_detected"
      - "specialized_knowledge_required"
      - "industry_specific_constraints"
      
    workflow:
      - identify_domain_experts: "find appropriate specialists"
      - generate_domain_questions: "domain-specific clarification"
      - consult_domain_knowledge: "leverage domain expertise"
      - validate_domain_clarification: "ensure domain accuracy"

# Learning and Adaptation
clarification_learning:
  enabled: true
  
  learning_algorithm:
    implementation: |
      # Clarification System Learning Algorithm
      INPUT: clarification_sessions, user_feedback, success_metrics
      OUTPUT: improved_question_generation, better_context_analysis
      
      LEARNING_COMPONENTS:
      1. Question Effectiveness Analysis:
          - Track which questions lead to successful clarification
          - Identify question patterns that work best for each domain
          - Optimize question sequencing and timing
          
      2. Response Pattern Recognition:
          - Learn from user response patterns
          - Improve response interpretation accuracy
          - Adapt to user communication preferences
          
      3. Domain Adaptation:
          - Refine domain-specific question templates
          - Improve domain terminology recognition
          - Optimize context sources for each domain
          
      4. Workflow Optimization:
          - Identify most effective clarification workflows
          - Adapt workflow selection criteria
          - Optimize escalation triggers and timing

# Quality Metrics
quality_metrics:
  clarification_success_rate:
    target: 0.90
    measurement: "percentage of clarifications that reduce ambiguity below 0.3"
    
  question_effectiveness:
    target: 0.85
    measurement: "percentage of questions that elicit useful responses"
    
  user_satisfaction:
    target: 0.80
    measurement: "user satisfaction with clarification process"
    
  efficiency_metrics:
    target_time: "< 5 minutes"
    target_questions: "< 7 questions"
    target_rounds: "< 3 rounds"

# Integration Points
integration:
  dependencies:
    - "config/knowledge-base/task-analysis.yaml" - for ambiguity detection
    - "config/dynamic/domain_system.yaml" - for domain-specific questions
    - "config/dynamic/runtime_environment.yaml" - for technical context
    - "config/knowledge-base/unified_agent_selection.yaml" - for workflow selection
    
  outputs:
    - "refined_task_context" - to task analysis and agent selection
    - "clarification_metrics" - to performance tracking
    - "user_feedback_data" - to learning systems
    - "escalation_requests" - to domain expert agents

# Error Handling
error_handling:
  unresponsive_user:
    detection: "no response within 5 minutes"
    recovery_strategies:
      - "simplify_questions: use more basic language"
      - "provide_examples: give concrete illustrations"
      - "offer_alternatives: present multiple choice options"
      - "escalate_to_human: involve human coordinator"
      
  circular_clarification:
    detection: "clarification_rounds > 3 without improvement"
    recovery_strategies:
      - "change_questioning_approach: use different question types"
      - "provide_recommendations: suggest likely interpretations"
      - "accept_partial_clarification: proceed with best understanding"
      - "escalate_to_expert: involve domain specialist"
      
  conflicting_responses:
    detection: "user provides contradictory information"
    recovery_strategies:
      - "identify_conflicts: point out specific contradictions"
      - "request_resolution: ask user to resolve conflicts"
      - "document_assumptions: proceed with documented assumptions"
      - "escalate_decision: involve human decision maker"

# Configuration Management
configuration:
  adaptive_parameters:
    question_complexity_threshold: 0.7
    max_questions_per_round: 7
    max_clarification_rounds: 3
    user_patience_factor: 0.8
    
  domain_specific_settings:
    technical_domains:
      question_style: "specific and detailed"
      context_sources: ["technical_documentation", "system_constraints"]
      
    creative_domains:
      question_style: "open and exploratory"
      context_sources: ["user_preferences", "creative_briefs"]
      
    business_domains:
      question_style: "goal-oriented and strategic"
      context_sources: ["business_requirements", "stakeholder_needs"]