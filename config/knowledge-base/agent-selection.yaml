# Agent Selection System
# Consolidated logic for intelligent agent selection and task assignment

metadata:
  name: "agent-selection-system"
  version: "1.0.0"
  description: "Intelligent agent selection with hybrid scoring and fallback strategies"
  author: "Master Agent System"

# Core Agent Selection Algorithm
agent_selection_algorithm:
  name: "hybrid_scoring_selection"
  description: "Select optimal agents using multiple scoring factors"
  
  input_schema:
    task_description:
      type: "string"
      description: "Natural language task description"
      required: true
    available_agents:
      type: "array"
      items:
        type: "object"
        properties:
          id: "string"
          capabilities: "array"
          performance_metrics: "object"
          current_load: "integer"
    task_context:
      type: "object"
      properties:
        complexity: "number"
        priority: "string"
        domain: "string"
        dependencies: "array"
  
  output_schema:
    selected_agents:
      type: "array"
      items:
        type: "object"
        properties:
          id: "string"
          confidence_score: "number"
          selection_reasoning: "string"
    selection_confidence: "number"
    alternative_agents: "array"
    error_status: "string"

# Scoring Components
scoring_components:
  tfidf_analysis:
    weight: 0.35
    description: "Text similarity analysis between task and agent capabilities"
    
    implementation: |
      # TF-IDF Similarity Analysis
      INPUT: task_description, agent_capabilities
      OUTPUT: relevance_scores (0.0-1.0)
      
      STEPS:
      1. Text preprocessing:
          - Extract keywords from task_description
          - Remove stop words (the, a, an, and, or, but, in, on, at, to, for, of, with, by)
          - Normalize text (lowercase, remove punctuation)
      2. Vocabulary building:
          - Create combined text from task + all agent capabilities
          - Build term frequency dictionary
          - Calculate inverse document frequency (IDF) for each term
      3. Vector creation:
          - Create TF-IDF vectors for task and each agent
          - Vector = (term_frequency * IDF) for each term in vocabulary
      4. Similarity calculation:
          - Calculate cosine similarity between task vector and agent vectors
          - Formula: similarity = (AÂ·B) / (|A| * |B|)
      5. Score normalization:
          - Normalize scores to 0.0-1.0 range
          - Apply confidence weighting
      
      FALLBACK: keyword matching if TF-IDF fails
      COMPLEXITY: O(n*m) where n=agents, m=vocabulary_size

  ml_prediction:
    weight: 0.30
    description: "Machine learning model for task-agent compatibility"
    
    implementation: |
      # ML Compatibility Prediction
      INPUT: task_features, agent_features, historical_data
      OUTPUT: compatibility_scores (0.0-1.0)
      
      ALGORITHM_TYPE: "Ensemble Learning"
      COMPONENTS:
      1. Feature extraction:
          - Task semantic features (domain, complexity, tools needed)
          - Agent capability features (skills, experience, specialization)
          - Historical performance data
      2. Model ensemble:
          - Competency matching model (skill-based scoring)
          - Performance prediction model (historical success rates)
          - Context adaptation model (similar task patterns)
      3. Weighted voting:
          - Combine predictions from all models
          - Apply confidence weights based on model accuracy
          - Final compatibility score calculation
      
      LEARNING_RATE: 0.1 (adaptive based on feedback)
      UPDATE_FREQUENCY: "weekly"
      MIN_TRAINING_SAMPLES: 50

  performance_history:
    weight: 0.25
    description: "Historical performance metrics and success rates"
    
    implementation: |
      # Performance-Based Scoring
      INPUT: agent_id, task_category, historical_performance
      OUTPUT: performance_scores (0.0-1.0)
      
      METRICS:
      1. Success rate:
          - successful_tasks / total_tasks
          - Weight: 0.4
          - Time window: last 100 tasks
      2. Response time:
          - inverse relationship with average response time
          - Target: < 60 seconds for optimal score
          - Weight: 0.3
      3. Quality score:
          - User satisfaction ratings
          - Output quality assessments
          - Weight: 0.3
      4. Reliability:
          - Task completion rate without failures
          - Error recovery success rate
          - Weight: 0.2
      
      CALCULATION:
      performance_score = (success_rate * 0.4) + 
                        (response_score * 0.3) + 
                        (quality_score * 0.3) + 
                        (reliability_score * 0.2)
      
      DECAY_FACTOR: 0.95 (monthly decay for older data)

  complexity_matching:
    weight: 0.10
    description: "Agent capability matching with task complexity"
    
    implementation: |
      # Complexity Compatibility Scoring
      INPUT: task_complexity, agent_capacity, agent_experience
      OUTPUT: compatibility_scores (0.0-1.0)
      
      COMPLEXITY_LEVELS:
      1.0: Simple tasks (0-2 steps, clear requirements)
      2.0: Moderate tasks (3-5 steps, some complexity)
      3.0: Complex tasks (6-10 steps, multiple components)
      4.0. Very complex tasks (10+ steps, enterprise level)
      5.0: Expert tasks (strategic, critical systems)
      
      CAPABILITY_MAPPING:
      - Simple tasks: General purpose agents
      - Moderate tasks: Specialized agents with domain expertise
      - Complex tasks: Expert-level agents
      - Very complex tasks: Senior architects with extensive experience
      - Expert tasks: Principal architects with strategic experience
      
      EXPERIENCE_BONUS:
      - Beginner: -0.2 score penalty
      - Intermediate: 0.0 base score
      - Advanced: +0.2 score bonus
      - Expert: +0.4 score bonus

# Selection Strategies
selection_strategies:
  default:
    name: "balanced_selection"
    description: "Balanced approach with all scoring factors"
    weights:
      tfidf: 0.35
      ml_prediction: 0.30
      performance_history: 0.25
      complexity_matching: 0.10
    selection_criteria: "highest_total_score"
    min_confidence: 0.6
    
  performance_priority:
    name: "performance_first"
    description: "Prioritize agents with best historical performance"
    weights:
      tfidf: 0.20
      ml_prediction: 0.25
      performance_history: 0.45
      complexity_matching: 0.10
    selection_criteria: "highest_performance_score"
    min_confidence: 0.7
    
  availability_priority:
    name: "available_first"
    description: "Prioritize agents that are immediately available"
    weights:
      tfidf: 0.25
      ml_prediction: 0.25
      performance_history: 0.20
      complexity_matching: 0.10
      availability_score: 0.20
    selection_criteria: "availability_and_performance"
    min_confidence: 0.5
    
  learning_priority:
    name: "learning_opportunity"
    description: "Prioritize agents that can learn from the task"
    weights:
      tfidf: 0.30
      ml_prediction: 0.35
      performance_history: 0.15
      complexity_matching: 0.10
      learning_potential: 0.10
    selection_criteria: "highest_learning_score"
    min_confidence: 0.4

# Selection Thresholds
selection_thresholds:
  minimum_confidence: 0.6
  max_candidates: 3
  min_relevance_score: 0.3
  max_task_complexity_mismatch: 1.5
  
  quality_gates:
    - All selected agents must meet minimum confidence threshold
    - At least one agent must exceed 0.8 confidence score
    - Total selection confidence must exceed 0.7 for critical tasks
    - Performance score must be > 0.6 for high-priority tasks

# Error Handling
error_handling:
  no_suitable_agents:
    detection: "all_candidates_score < minimum_confidence"
    recovery_strategies:
      - "lower_confidence_threshold: 0.1"
      - "expand_search_criteria: include_related_capabilities"
      - "request_human_intervention: clarify requirements"
      - "use_general_purpose_agent: fallback_option"
    
  ml_model_failure:
    detection: "ml_prediction_model_unavailable_or_error"
    recovery_strategies:
      - "fall_back_to_hybrid_scoring: tfidf + performance"
      - "use_historical_patterns_only"
      - "implement_rule_based_selection"
      
  tfidf_processing_error:
    detection: "tfidf_calculation_fails_or_timeout"
    recovery_strategies:
      - "use_keyword_matching_fallback"
      - "simplify_task_description"
      - "use_agent_capabilities_matching"
      
  data_corruption:
    detection: "invalid_or_missing_input_data"
    recovery_strategies:
      - "use_cached_selection_results"
      - "request_data_refresh"
      - "implement_emergency_selection"

# Output Processing
output_processing:
  result_format:
    primary_selection:
      - agent_id: "string"
      - confidence_score: "number"
      - selection_factors: "object"
      - reasoning: "string"
    
    alternatives:
      - agent_id: "string"
      - confidence_score: "number"
      - selection_factors: "object"
      - reasoning: "string"
  
  confidence_calculation:
    method: "weighted_average_with_uncertainty"
    uncertainty_factor: "0.1"
    min_confidence: "0.3"
    max_confidence: "0.95"
  
  validation_checks:
    - selected_agents_exist_in_available_list
    - confidence_scores_within_valid_range
    - selection_reasoning_is_coherent
    - no_conflicts_between_selections

# Integration Points
integration:
  imports:
    - "task_analysis.task_complexity_classifier"
    - "tfidf_system.text_similarity_analyzer"
    - "learning_module.performance_predictor"
  
  exports:
    - "agent_selection_results"
    - "selection_confidence_scores"
    - "agent_performance_updates"
    - "selection_learning_data"

# Performance Metrics
performance_metrics:
  selection_accuracy: 0.85
  average_response_time: 2.5  # seconds
  fallback_usage_rate: 0.15
  user_satisfaction_target: 0.90
  
  optimization_targets:
    - reduce_selection_time_to_under_2_seconds
    - increase_selection_accuracy_to_above_0.90
    - minimize_fallback_usage_to_below_0.10
    - achieve_user_satisfaction_above_0.95